\documentclass[10pt]{article}
%\usepackage[utf8]{inputenc}
\usepackage{amssymb} % for "real numbers" sign
\usepackage{amsmath}  % for align environment
\usepackage{hyperref}
\usepackage[top=60pt,bottom=60pt,left=96pt,right=92pt]{geometry}
\usepackage{xcolor}

\begin{document}
%-------------------------------------------------------------------------------------------------------%
\title{Sensitivities via adjoints}
\author{Tobias Kattmann}
\maketitle
\tableofcontents

%-------------------------------------------------------------------------------------------------------%
\section{Primal}
%-------------------------------------------------------------------------------------------------------%
\subsection{Primal equation}
The heat equation in 3D is a parabolic PDE:
\begin{equation}\label{eq:heat3D}
\frac{\partial u}{\partial t} + \nabla\cdot \left(- d\nabla u \right) = q.
\end{equation}
The task is to find a solution $u\left(\mathbf{x}, t\right)\in\mathbb{R}$ of  (\ref{eq:heat3D}) for $\mathbf{x}\in\mathbb{R}^3$ and $t\in(0,T]$. The inital and boundary conditions are:
\begin{align}
u(\mathbf{x},0) & = u_{init}(\mathbf{x}), \\
u(\Gamma,t) & = u_{BC}(\mathbf{x,t}), \quad \mathbf{x}\in\Gamma
\end{align}
The parameter $d$ is the thermal diffusivity (physical meaning) and is here considered as a function of space $d\left(\mathbf{x}\right)$. In 1D, the IBVP (Inital Boundary Value Problem) writes:
\begin{eqnarray}
\frac{\partial u}{\partial t} +\frac{\partial}{\partial x} \left(- d \frac{\partial u}{\partial x}  \right) &=& 0, \\
\text{subject to} \\
u(x,0) &=& u_{init}(x), \\
u(x_{start},t)&=& u_{start}, \\
u(x_{end},t) &=& u_{end}.
\end{eqnarray}
\subsubsection{Analaytical solution (fundamental sol)}
In order to validate the primal solution one can use the fundamental solution to the Diffusion problem. The following is derived with the help of MIT MATH 18.152 notes \href{http://math.mit.edu/~jspeck/18.152_Fall2011/Lecture%20notes/18152%20lecture%20notes%20-%205.pdf}{Link} by Prof Speck. The integral at the end is solved with wolframalpha.com \href{https://sandbox.open.wolframcloud.com/app/objects/a62c75f0-0ef8-44e4-8ba9-4eedb585f643#sidebar=compute}{Link}.
	
The 1D heat equation in our case writes:
\begin{equation}
u_t-u_{xx} = 0,\quad u(x,0) = -exp\left( -(y-15)^2 \right)
\end{equation}
Starting from Theorem 1.1 from the course notes one simply needs to compute equation (1.1.12) which is done here by using wolframalpha and $g(y)=exp\left( -y^2 \right)$. This gives the solution:
\begin{equation}
u(x,t) = \frac{1}{(4\pi d t)^{1/2}} \sqrt{\pi} \sqrt{\frac{a}{1+a}}exp\left( -\frac{x^2}{1+a} \right),\quad a= 4dt
\end{equation}
For $g(y)=exp\left( -(y-15)^2 \right)$ one has:
\begin{align}
u(x,t) &= \frac{1}{(4\pi d t)^{1/2}} \sqrt{\pi} \sqrt{\frac{a}{1+a}}exp\left( -\frac{(x-15)^2}{1+a} \right),\quad a= 4dt \\
u(x,t) &= \frac{1}{\sqrt{1+4dt}}exp\left(-\frac{(x-15)^2}{1+4dt} \right))
\end{align}
And for later use, the continuous adjoint equation is:
\begin{equation}
-\lambda_t-\lambda_{xx} = 0,\quad \lambda(x,T) = -exp\left( -(y-20)^2 \right)
\end{equation}
When readjusting the temporal propagation one gets:
\begin{equation}
\lambda_t-\lambda_{xx} = 0,\quad \lambda(x,0) = -exp\left( -(y-20)^2 \right)
\end{equation}
which again can be solved with the presented approach using $g(y)=-exp\left( -(y-20)^2 \right)$:
\begin{equation}
\lambda(x,t) = \frac{-1}{\sqrt{1+4dt}}exp\left(-\frac{(x-20)^2}{1+4dt} \right))
\end{equation}

%-------------------------------------------------------------------------------------------------------%
\subsection{Discretization of primal}
We want to solve the transient 1D Diffusion equation numerically with Finite Differences (FD). We will use the Method Of Lines (MOL) with a dual time stepping approach. Therefore we need to discretize the spatial and temporal domain such that $x=(x_i), i=0..m-1$ () and $t=(t_i), i=0 .. n-1$. That means we have $m$ grid points and $m-2$ DOF's as we have Dirichlet boundary conditions on the two outer grid points. There are $n$ time level where $n-1$ have to be computed as the initial solution is given. We rewrite the PDE by using the product rule of differentiation:
\begin{equation}
\frac{\partial u}{\partial t} - \frac{\partial d}{\partial x}\frac{\partial u}{\partial x} -d \frac{\partial^2 u}{\partial x^2}  = 0
\end{equation}
We set up an equation for each of the $m-2$ DOF's, as the values at the Dirichlet points $u_0, u_{n-1}$ are already known. First order spatial derivatives in the second term are discretized using central differences.
\begin{equation}
\frac{\partial u_i}{\partial x} = \frac{u_{i+1} - u_{i-1}}{2\Delta x}
\end{equation}
Second order spatial derivatives in the third term are discretized using central differences.
\begin{equation}
\frac{\partial^2 u_i}{\partial x^2} = \frac{u_{i+1} - 2 u_{i} + u_{i-1}}{\Delta x^2}
\end{equation}
Both methods can be expressed as an operator in form of a matrix that is applied on the quantity that needs to be derived. Say, First order derivative is expressed as $\mathbf{A_1}$ and second order as $\mathbf{A_2}$:
\begin{equation}
\frac{\partial u}{\partial t} - -diag(\mathbf{A_1}d)\mathbf{A_1}u - diag(d) \mathbf{A_2}u = 0
\end{equation}
As the spatial discretization is linear in $u$ we collect the terms and simplify towards a single fixed operator working on u:
\begin{align}
&-diag(\mathbf{A_1}d)\mathbf{A_1}u - diag(d) \mathbf{A_2}u \\
=&\left[ -diag(\mathbf{A_1}d)\mathbf{A_1} - diag(d) \mathbf{A_2} \right]u \\
=& \mathbf{B}u
\end{align}
For the MOL the spatial discretization terms are gathered in a Term $R(u,d)$.
\begin{align}
&R(u,d) = \mathbf{B}u  \\
\Rightarrow & \frac{\partial u}{\partial t} + R(u,d) = 0
\end{align}
The time discretization is done using first order backward differences:
\begin{equation}
\hat R (u,d) = \frac{u^{n} - u^{n-1}}{\Delta t} + R(u^n,d) = 0,\quad n= 1\,..\,N
\end{equation}
With $N$ being the number of physical time steps. $u^0$ is given as the initial solution as part of the IBVP. The dual time stepping method is then used to converge the above equation to a steady state in fictitious time $\tau$. 
\begin{equation}
\frac{d u^n}{d\tau} + \hat R(u^{n},d) = 0
\end{equation} 
The explicit Euler method is used here. Each iteration of the pseudo time stepping is depicted with the subscript $p$:
\begin{equation}
\frac{u^n_{p+1}-u^n_{p}}{\Delta\tau} + \hat R(u^{n}_{p},d) = 0,\quad p= 1\,..\,m,\quad n= 1\,..\,N
\end{equation} 
This can be rewritten in a fixed-point iteration form:
\begin{align}
u^n_{p+1} &= u^n_{p} - \Delta\tau\hat R(u^{n}_{p},d),\quad p= 1\,..\,m,\quad n= 1\,..\,N\\
u^n_{p+1} &= G^n \left( u^n_p, u^{n-1} \right),\quad p= 1\,..\,m,\quad n= 1\,..\,N
\end{align}
where $u^{n-1}$ is the converged solution of the previous time step. The fixed point iteration converges to the solution (fixed point) $u^n$:
\begin{equation}
u^n = G^n\left( u^n, u^{n-1} \right),\quad n= 1\,..\,N
\end{equation}
%-------------------------------------------------------------------------------------------------------%
\subsection{Optimization task}
The optimization Task is:
\begin{align}
\underset{\alpha}{min}\quad J &= J(u^N), \\
\text{subject to}\quad u^n &= G^n \left( u^n, u^{n-1}, \alpha \right),\quad n= 1\,..\,N
\end{align}
The objective function is only dependent on the solution at the final time. This decision will have impact during the derivation of the adjoint equation, the derivative of the objective function with respect to the state variable will appear as an initial condition for the adjoint solution instead of a source term in the adjoint equation:
\begin{equation}
J(u^N) = -(u^N)^T f(x)
\end{equation}
The negative sign indicates that the actually a maximization is desired by with that the widely used understanding that optimization equals minimization we are consistent with that. 
The objective function can be understood as spatial integration of solution with a function $f$.
%-------------------------------------------------------------------------------------------------------%
\subsection{Lagrangian of the constrained optimization problem}
The Lagrangian of the above formulated constrained optimization problem is:
\begin{equation}
\mathcal{L} = J\left( u^N \right) - \sum_{n=1}^{N} \left( \lambda^n \right)^T\left( u^n - G^n \left( u^n, u^{n-1}, \alpha \right) \right)
\end{equation}
with $\lambda^n$ the Lagrangian multiplier vector (or adjoint state vector) at time $n$. The first order optimality conditions are:
\begin{align}
\frac{\partial \mathcal{L}}{\partial \lambda^n} &= 0,\quad n= 1\,..\,N\quad\text{(state equations)}\\
\frac{\partial \mathcal{L}}{\partial u^n} &= 0,\quad n= 1\,..\,N\quad\text{(adjoint equations)}\\
\frac{\partial \mathcal{L}}{\partial \alpha} &= 0,\quad\text{(control equation)}\\
\end{align}
%-------------------------------------------------------------------------------------------------------%
\subsection{Derivation of adjoint equations}
With above formulated optimality conditions we can directly derive the adjoint equations in a fixed point form. Otherwise one would take the total derivative of $\mathcal{L}$ with respect to $\alpha$ and gather the terms containing du/da.
\begin{equation}
\frac{\partial \mathcal{L}}{\partial u^n} = \frac{\partial J}{\partial u^n} - \left( \lambda^n \right)^T +\left( \lambda^n \right)^T \frac{\partial G^n \left( u^n,u^{n-1} \right)}{\partial u^n} +\left( \lambda^{n+1} \right)^T \frac{\partial G^{n+1}\left( u^{n+1},u^{n} \right)}{\partial u^n}  = 0
\end{equation}
Taking the transpose and reformatting in a fixed point form leads to:
\begin{equation}
\lambda^{n}_{i+1} = \left( \frac{\partial J}{\partial u^n} \right)^T + \left( \frac{\partial G^n\left( u^n,u^{n-1} \right)}{\partial u^n}  \right)^T \lambda^n_i  +\left( \frac{\partial G^{n+1}\left( u^{n+1},u^{n} \right)}{\partial u^n}  \right)^T \lambda^{n+1}
\end{equation}
with subscript $i$ indicating inner iterations. In the following the each term is looked at in detail and derived from the actual primal discretization as until now everything is as general as it gets. An iteration procedure is the goal. 
%-------------------------------------------------------------------------------------------------------%
\subsubsection*{1st term}
The first term ist zero except for the last time step $N$:
\begin{align}
\left( \frac{\partial J}{\partial u^n} \right)^T =& 0,\quad n = 1\,..\,N-1 \\
\left( \frac{\partial J}{\partial u^N} \right)^T =& -f(x) 
\end{align}
%-------------------------------------------------------------------------------------------------------%
\subsubsection*{2nd term}
The following derivation is valid for $n = 1\,..\,N$. Starting with:
\begin{equation}
G^n\left( u^n,u^{n-1} \right) = u^n - \Delta\tau\left[ \frac{1}{\Delta t} u^{n} - \frac{1}{\Delta t} u^{n-1} + R \left( u^n \right) \right]
\end{equation}
wherein we can express the spatial residual very simple as a linear matrix-vector product:
\begin{equation}
G^n\left( u^n,u^{n-1} \right) = u^n - \Delta\tau\left[ \frac{1}{\Delta t} u^{n} - \frac{1}{\Delta t} u^{n-1} + \mathbf{B}u^n  \right]
\end{equation}
Then the derivative is:
\begin{equation}
\frac{\partial G^n\left( u^n,u^{n-1} \right)}{\partial u^n} = \mathbf{I} - \Delta\tau\left[ \frac{1}{\Delta t} \mathbf{I} + \mathbf{B} \right]
\end{equation}
The term as is stands in above formula then is:
\begin{equation}
\left( \frac{\partial G^n\left( u^n,u^{n-1} \right)}{\partial u^n} \right)^T \lambda^n = \lambda^n - \Delta\tau\left[ \frac{1}{\Delta t} \lambda^n + \mathbf{B}^T \lambda^n \right]
\end{equation}
%-------------------------------------------------------------------------------------------------------%
\subsubsection*{3rd term}
The following derivation is valid for $n = 1\,..\,N-1$. Starting with:
\begin{equation}
G^{n+1}\left( u^{n+1},u^{n} \right) = u^{n+1} - \Delta\tau\left[ \frac{1}{\Delta t} u^{n+1} - \frac{1}{\Delta t} u^{n} + R \left( u^{n+1} \right) \right]
\end{equation}
Then the derivative is:
\begin{equation}
\frac{\partial G^{n+1}\left( u^{n+1},u^{n} \right)}{\partial u^n} =  \Delta\tau\left[ -\frac{1}{\Delta t} \mathbf{I} \right]
\end{equation}
The term as is stands in above formula then is:
\begin{equation}
\left( \frac{\partial G^{n+1}\left( u^{n+1},u^{n} \right)}{\partial u^n} \right)^T \lambda^{n+1} =   \Delta\tau\left[ -\frac{1}{\Delta t} \lambda^{n+1} \right]
\end{equation}
The third term is non-existent at timestep $N$. To be more specific, there is no $G^{N+1}$ as $N$ is the last timestep of the primal. But from the first term there is a contribution to the adjoint fixed point iteration at time $N$. This can be interpreted as an initial condition:
\begin{equation}
\lambda^{N}_{i+1} = \left( \frac{\partial J}{\partial u^N} \right)^T + \left( \frac{\partial G^N\left( u^N,u^{N-1} \right)}{\partial u^N}  \right)^T \lambda^N_i  
\end{equation}
%-------------------------------------------------------------------------------------------------------%
\subsubsection*{Recombined}
The recovered fixed point iteration is:
\begin{align}
\lambda^{N}_{i+1} &= \left( \frac{\partial J}{\partial u^N} \right)^T + \left( \frac{\partial G^N\left( u^N,u^{N-1} \right)}{\partial u^N}  \right)^T \lambda^N_i ,\quad i = 1\,..\,m \\
\lambda^{n}_{i+1} &=  \left( \frac{\partial G^n\left( u^n,u^{n-1} \right)}{\partial u^n}  \right)^T \lambda^n_i  +\left( \frac{\partial G^{n+1}\left( u^{n+1},u^{n} \right)}{\partial u^n}  \right)^T \lambda^{n+1} ,\quad i = 1\,..\,m,\quad n = N-1\,..\, 1
\end{align}
The index i counts the internal (pseudo) iterations. Now the terms are expanded:
\begin{align}
\lambda^{N}_{i+1} &= -f(x) + \lambda^n_i - \Delta\tau\left[ \frac{1}{\Delta t} \lambda^n_i + \mathbf{B}^T \lambda^n_i \right] ,\quad i = 1\,..\,m \\
\lambda^{n}_{i+1} &= \lambda^n_i - \Delta\tau\left[ \frac{1}{\Delta t} \lambda^n_i + \mathbf{B}^T \lambda^n_i \right] + \Delta\tau\left[ -\frac{1}{\Delta t} \lambda^{n+1} \right]  ,\quad i = 1\,..\,m ,\quad n = N-1\,..\, 1
\end{align}
Reformulating:
\begin{align}
\lambda^{N}_{i+1} &= \lambda^n - \Delta\tau\left[ \frac{1}{\Delta t} \lambda^n - \frac{1}{\Delta t} \left( - \frac{\Delta t}{\Delta\tau} f(x) \right) + \mathbf{B}^T \lambda^n \right]  \\
\lambda^{n}_{i+1} &= \lambda^n - \Delta\tau\left[ \frac{1}{\Delta t} \lambda^n - \frac{1}{\Delta t} \lambda^{n+1} + \mathbf{B}^T \lambda^n \right]  ,\quad n = N-1\,..\, 1
\end{align}
The derivation of the obj func at state $N$ can therefore be seen as an initial condition to the time stepping of the adjoint:
\begin{equation}
\lambda^{N+1} = -f(x),
\end{equation}
considering that we choose $\Delta\tau=\Delta t$ which is valid if the fixed point is reached.
\subsubsection*{Gradient from adjoints}
Now that the adjoint state is known for each time step the gradient of the Lagrangian wrt to the design variables can be computed:
\begin{equation}
\frac{d \mathcal L}{d \alpha} = \frac{\partial J}{\partial \alpha} + \sum_{n=1}^{N} \left( \lambda^n \right)^T \frac{\partial G^n}{\partial \alpha}.
\end{equation}
In our case the first term equals to zero such that:
\begin{equation}
\frac{d \mathcal L}{d \alpha} =  \sum_{n=1}^{N} \left( \lambda^n \right)^T \frac{\partial G^n}{\partial \alpha},
\end{equation}
where the last term computes to:
\begin{align}
\frac{\partial G^n}{\partial \alpha} &= \frac{\partial}{\partial \alpha}\left[ u^n - \Delta\tau\left[ \frac{1}{\Delta t} u^{n} - \frac{1}{\Delta t} u^{n-1} + \mathbf{B}u^n  \right] \right] \\
\frac{\partial G^n}{\partial \alpha} &= \frac{\partial}{\partial \alpha}\left[ u^n - \Delta\tau\left[ \frac{1}{\Delta t} u^{n} - \frac{1}{\Delta t} u^{n-1} -diag(\mathbf{A_1}d)\mathbf{A_1}u - diag(d) \mathbf{A_2}u  \right] \right]\\
\frac{\partial G^n}{\partial \alpha} &= \frac{\partial}{\partial \alpha}\left[ u^n - \Delta\tau\left[ \frac{1}{\Delta t} u^{n} - \frac{1}{\Delta t} u^{n-1} -diag(\mathbf{A_1}u)\mathbf{A_1}d - diag(\mathbf{A_2}u) d  \right] \right] \\
\frac{\partial G^n}{\partial \alpha} &= -\Delta\tau \left(-diag(\mathbf{A_1}u)\mathbf{A_1} - diag(\mathbf{A_2}u) \right)
\end{align}
Note that our above written gradient of the Lagrangian is a row vector. To get a column vector we take the transpose of the equation:
\begin{equation}
\frac{d \mathcal L}{d \alpha}^T =  \sum_{n=1}^{N} \left( \frac{\partial G^n}{\partial \alpha} \right)^T \lambda^n,
\end{equation}
Inserting the formulation for the fixed point iterator yields:
\begin{equation}
\frac{d \mathcal L}{d \alpha}^T =  \sum_{n=1}^{N} \left[-\Delta\tau \left( -diag(\mathbf{A_1}u)\mathbf{A_1} - diag(\mathbf{A_2}u) \right) \right]^T \lambda^n,
\end{equation}
And by construction via Lagrangian we have the sensitivities of the objective function wrt to the design variables:
\begin{equation}
\frac{d J}{d \alpha} =\frac{d \mathcal L}{d \alpha}
\end{equation}
%-------------------------------------------------------------------------------------------------------%
\subsection{Update of the design variables}
The optimization is performed using a gradient based approach, where the design variables are updated:
\begin{equation}
\alpha_{i+1} = \alpha_i - \epsilon \frac{d J}{d \alpha}
\end{equation}
The approach to calculate the derivative of the objective function with respect to the design variables is presented in the following section.
%-------------------------------------------------------------------------------------------------------%
\subsection{Derivation of discrete adjoint equation}
In order to derive the adjoint equations we construct the Lagrangian $\mathcal{L}$ corresponding to the optimization problem:
\begin{equation}
\mathcal{L} = \int_{0}^{T} j\left(x,u,\alpha,t\right) +\lambda^T h(x,u,\dot u, \alpha,t) dt +\mu^T g(x(0),\alpha).
\end{equation}
\textcolor{red}{text}
\begin{equation}
J = J - \int_{0}^{T} \int_{\Omega} \lambda h \,dx\,dt
\end{equation}
\begin{equation}
J = J - \int_{0}^{T} \lambda^T h \,dt
\end{equation}
\textcolor{red}{text}
with $h(u,\dot u, \alpha,t)$ beeing:
\begin{equation}
h(x,\dot x, \alpha,t) = \frac{\partial u}{\partial t} + R\left(u, \alpha \right) = 0,
\end{equation}
and the initial conditions
\begin{equation}
g(u(0),\alpha) = 0.
\end{equation}
The vector of Lagrangian multipliers (i.e. we are speaking of discretized version) $\lambda$ is a function of time, and $\mu$ is another vector of multipliers that are associated with the initial conditions. Because the two constraints $h = g = 0$ we are free to set the values of $\lambda$ and $\mu$, abd $d_{\alpha}\mathcal{L} = d_{\alpha}J$. Taking the total derivative of $\mathcal{L}$:
\begin{align}
\frac{d\mathcal{L}}{d \alpha} = \int_{0}^{T}\left[\frac{\partial j}{\partial u} \frac{\partial u}{\partial \alpha} + 
\frac{\partial j}{\partial \alpha} +
\lambda^T \left( \frac{\partial h}{\partial u} \frac{\partial u}{\partial \alpha} + \frac{\partial h}{\partial\dot u} \frac{\partial\dot u}{\partial \alpha} +
\frac{\partial h}{\partial \alpha} \right)\right] dt + \\
\mu^T \left( \frac{\partial g}{\partial u(0)} \frac{\partial u(0)}{\partial\alpha}  + \frac{\partial g}{\partial\alpha} \right)
\end{align}
The task is now find a form of $d_{\alpha}\mathcal{L}$ such that all $\partial_{\alpha} u$ and $\partial_{\alpha}\dot u$ terms are eliminated as they are costly to evaluate. First, in a sidestep, we integrate the term containing $\dot u$ by parts:
\begin{equation}
\int_{0}^{T} \lambda^T \frac{\partial h}{\partial\dot u} \frac{\partial\dot u}{\partial \alpha} dt =
\left[ \lambda^T \frac{\partial h}{\partial\dot u} \frac{\partial u}{\partial \alpha} \right]^{T}_{0} - 
\int_{0}^{T} \left[\dot \lambda^T \frac{\partial h}{\partial\dot u} + \lambda^T \frac{d}{dt}\frac{\partial h}{\partial\dot u} \right] 
\frac{\partial u}{\partial \alpha} dt
\end{equation}
Substituting the result of integrating by parts into the original Lagrangian derivation and reformatting:
\begin{align}
\frac{d\mathcal{L}}{d \alpha} = 
&\int_{0}^{T}\left( \frac{\partial j}{\partial u} + \lambda^T \left(  \frac{\partial h}{\partial u} + \frac{d}{dt}\frac{\partial h}{\partial\dot u} \right) - \dot\lambda^T \frac{\partial h}{\partial\dot u} \right) \frac{\partial u}{\partial \alpha} dt \\
+&\int_{0}^{T}\frac{\partial j}{\partial \alpha} + \lambda^T \frac{\partial h}{\partial \alpha} dt \\
+& \left[ \lambda^T \frac{\partial h}{\partial\dot u} \frac{\partial u}{\partial \alpha} \right]^{T} \\
+& \left[ \lambda^T \frac{\partial h}{\partial\dot u} + \mu^T \frac{\partial g}{\partial u(0)} \right]_{0}  \frac{\partial u(0)}{\partial \alpha} \\
+& \mu^T\frac{\partial g}{\partial\alpha}
\end{align}
In order to get avoid the need of computing $\partial_{\alpha} u$ we require in the first term for $t>0$:
\begin{equation}
\frac{\partial j}{\partial u} + \lambda^T \left(  \frac{\partial h}{\partial u} + \frac{d}{dt}\frac{\partial h}{\partial\dot u} \right) - \dot\lambda^T \frac{\partial h}{\partial\dot u} = 0.
\end{equation}
We reformulate the adjoint equation by taking the transpose of the whole equation and multiplying with $-1$:
\begin{equation}
\frac{\partial h}{\partial\dot u}^T\dot\lambda - \left(\frac{\partial h}{\partial u} + \frac{d}{dt}\frac{\partial h}{\partial\dot u} \right)^T \lambda  - \frac{\partial j}{\partial u}^T  = 0.
\end{equation}
For every PDE we can simply require that $\frac{\partial h}{\partial\dot u} = I$ with $I$ being the elementary matrix such that the adjoint PDE simplifies to:
\begin{equation}
\dot\lambda - \frac{\partial h}{\partial u}^T \lambda  - \frac{\partial j}{\partial u}^T  = 0.
\end{equation}
\begin{equation}
\dot\lambda - \frac{\partial h}{\partial u}^T \lambda = 0.
\end{equation}
\begin{align}
\lambda(T) &= -e^{-(x-20)^2},\\
\lambda(x_{start},t) &= 0,\\ 
\lambda(x_{end},t) &= 0 
\end{align}
\begin{align}
\lambda(T) &= -e^{-(x-20)^2},\\
\lambda(x_{start},t) &= 0,\\ 
\lambda(x_{end},t) &= 0 
\end{align}
\begin{align}
\lambda(T) &= -\frac{\partial J}{\partial u}
\end{align}
where the $\frac{\partial h}{\partial u}^T$ represents a matrix operator which works on $\lambda$. \textcolor{red}{What about the time marching algorithm, can it be chosen freely or is it also a derivative of the primal? If so where is the error in the derivation.}
Furthermore for the third term we set $\lambda(T)=0$. \textcolor{red}{What about BC?! Find appropriate derivation. Or is it already incorporated through discrete operators} Finally we set for the fourth term to zero using:
\begin{equation}
\mu^T = \left[- \lambda^T \frac{\partial h}{\partial\dot u} \right]_0 \left( \frac{\partial g}{\partial u(0)}\right)^{-1}
\end{equation}
The final total derivative then writes:
\begin{equation}
\frac{dJ}{d \alpha} =  \frac{d\mathcal{L}}{d \alpha} = \int_{0}^{T}\frac{\partial j}{\partial \alpha} + \lambda^T \frac{\partial h}{\partial \alpha} dt + \left[- \lambda^T \frac{\partial h}{\partial\dot u} \right]_0 \left( \frac{\partial g}{\partial u(0)}\right)^{-1} \frac{\partial g}{\partial\alpha}
\end{equation}
In our case we have $\frac{\partial g}{\partial\alpha}=0$ and $\frac{\partial j}{\partial \alpha}=0$ such that the total derivative simplifies to:
\begin{equation}
\frac{dJ}{d \alpha} =  \frac{d\mathcal{L}}{d \alpha} = \int_{0}^{T} \lambda^T \frac{\partial h}{\partial \alpha} dt
\end{equation}
\begin{equation}
\frac{dJ}{d \alpha} = \int_{0}^{T} \lambda^T \frac{\partial h}{\partial \alpha} dt
\end{equation}
%-------------------------------------------------------------------------------------------------------%
\newpage
\section{Additional Material}
From Qiqi Wang's youtube channel. Lecture 18 from 2015
\subsection{Steady state discrete adjoint equation}
Consider the optimization problem:
\begin{eqnarray}
\underset{\alpha}{min}\,J(u,\alpha)\quad \text{subject to}\\
R(u,\alpha)=0
\end{eqnarray}
The functions $J$ and $R$ can be nonlinear in this case. We construct the Lagrangian:
\begin{equation}
J(u,\alpha) = J(u,\alpha) + 0 = J(u,\alpha) + R(u,\alpha) =  J(u,\alpha) + \lambda^T R(u,\alpha) = \mathcal{L},
\end{equation}
where we can freely choose the Lagrange multipliers $\lambda$. Note that everything is represented in its discrete version. So $R$ is a vector of the dimension of the unknowns. The total differential (here we assume $J = J(u)$) writes:
\begin{equation}
d\mathcal{L} = \frac{\partial J}{\partial u} du + \lambda^T \left( \frac{\partial R}{\partial u} du + \frac{\partial R}{\partial \alpha} d\alpha\right)
\end{equation}
which is the same as:
\begin{equation}
\frac{d\mathcal{L}}{d\alpha} = \frac{\partial J}{\partial u} \frac{du}{d\alpha} + \lambda^T \left( \frac{\partial R}{\partial u} \frac{du}{d\alpha} + \frac{\partial R}{\partial \alpha}\right)
\end{equation}
Eliminating the term $du$ or $\frac{du}{d\alpha}$ respectively yields the adjoint equation:
\begin{equation}
d\mathcal{L} = \underbrace{\left(\frac{\partial J}{\partial u} + \lambda^T  \frac{\partial R}{\partial u} \right)}_{\overset{!}{=} 0} du + \lambda^T \frac{\partial R}{\partial \alpha} d\alpha
\end{equation}
The adjoint equation is under-braced. This leaves the sensitivities:
\begin{equation}
\frac{d\mathcal{L}}{d\alpha} = \frac{dJ}{d\alpha} = \lambda^T \frac{\partial R}{\partial \alpha}
\end{equation}
\textcolor{red}{Are the boundary conditions decoded in the adjoint equation over $R$?}
%-------------------------------------------------------------------------------------------------------%
\subsection{Continuous Adjoint: Poisson equation}
Consider the optimization problem:
\begin{eqnarray}
\underset{\alpha}{min}\,J(u)=\int_{\Omega}cu \,dx\quad \text{subject to}\\
R(u,\alpha)= \nabla\cdot\left( \alpha \nabla u \right) - f =0, \quad u=0\,on \,\partial\Omega
\end{eqnarray}
where c is an arbitrary function, $\alpha = \alpha(x)$. Again we're adding zero to our objective function aka defining the Lagrangian:
\begin{equation}
J = \int_{\Omega}cu \,dx + \int_{\Omega} \lambda\left( \nabla\cdot\left( \alpha \nabla u \right) - f \right) \,dx
\end{equation}
The total differential writes
\begin{equation}
\delta J = \int_{\Omega}c\,\delta u \,dx + \int_{\Omega} \lambda\left( \nabla\cdot\left( \alpha \nabla \delta u \right)  \right) \,dx + \int_{\Omega} \lambda\left( \nabla\cdot\left( \delta\alpha \nabla  u \right)  \right) \,dx
\end{equation}
As we want to avoid computing $\delta u$ we perform integration by parts twice on the middle term to receive a term without applied derivations:
\begin{equation}
\int_{\Omega} \lambda\left( \nabla\cdot\left( \alpha \nabla \delta u \right)  \right) \,dx = \int_{\partial\Omega} \lambda \left( \alpha \nabla \delta u \right)\cdot n \,dS - \int_{\Omega} \nabla\lambda \cdot \left( \alpha \nabla \delta u \right) \,dx
\end{equation}
where $n$ is the outward facing normal vector. We set $\lambda=0$ on the boundary $\partial\Omega$ such that the surface Integral vanishes. We continue with the second integration by parts, the factor $\alpha$ can be repositioned:
\begin{equation}
- \int_{\Omega} \left(\alpha\nabla\lambda\right) \cdot \nabla \delta u \,dx = -\int_{\partial\Omega}\delta u\left(\alpha\nabla\lambda\right) \cdot n \,dS + \int_{\Omega}\delta u\left(\nabla \cdot \left(\alpha\nabla\lambda\right)\right) \,dx
\end{equation}
where the boundary integral is zero everywhere as $\lambda=0$ on $\partial\Omega$. Putting the leftover term back in the total differential yields:
\begin{align}
\delta J &= \int_{\Omega}c\,\delta u \,dx + \int_{\Omega}\delta u\left(\nabla \cdot \left(\alpha\nabla\lambda\right)\right) \,dx + \int_{\Omega} \lambda\left( \nabla\cdot\left( \delta\alpha \nabla  u \right)  \right) \,dx\\
&= \int_{\Omega} \underbrace{\left[c + \nabla \cdot \left(\alpha\nabla\lambda\right)\right]}_{\overset{!}{=} 0} \delta u \,dx + \int_{\Omega} \lambda\left( \nabla\cdot\left( \delta\alpha \nabla  u \right)  \right) \,dx \\
&= \int_{\Omega} \lambda\left( \nabla\cdot\left( \delta\alpha \nabla  u \right)  \right) \,dx 
\end{align}
Performing now another integration by parts yields:
\begin{align}
\delta J &= \underbrace{\int_{\partial\Omega} \lambda \left( \delta\alpha \nabla  u \right)\cdot n \,dS}_{=0} - \int_{\Omega} \nabla\lambda \cdot \left( \delta\alpha \nabla u \right) \,dx\\
&= - \int_{\Omega} \nabla\lambda \cdot \nabla u \,\delta\alpha \,dx
\end{align}
\textcolor{red}{The sign is wrong in Qiqi's derivation's last step. Now Qiqi comes to the conclusion which I cannot follow. He simply says that then:}
\begin{equation}
\frac{\partial J}{\partial \alpha} = \nabla\lambda\cdot \nabla u
\end{equation}
%-------------------------------------------------------------------------------------------------------%
\end{document}
